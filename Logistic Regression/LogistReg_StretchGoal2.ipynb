{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LogistReg_StretchGoal2.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"SCxkRdqE40SQ"},"source":["# **Multinomial regression on Iris dataset**\n","\n","**By Philip Blumin and Paul Cucchiara**"]},{"cell_type":"markdown","metadata":{"id":"-PUkGBJ7zXC3"},"source":["# **Dataset info** "]},{"cell_type":"markdown","metadata":{"id":"gRe4ReVJzQHy"},"source":["This Iris database was obtained from the University of California Irvine Machine Learning Repository from R.A. Fisher. \n","\n","Attributes:\n","1. **SL:** sepal length in cm\n","2. **SW:** sepal width in cm\n","3. **PL:** petal length in cm\n","4. **PW:** petal width in cm\n","\n","Classes:\n","\n","-- Iris Setosa - 0\n","\n","-- Iris Versicolour - 1\n","\n","-- Iris Virginica - 2\n","\n","*Numbers indicate associated value in one hot encoded matrix\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xdDwuyenuXVJ"},"source":["# **Stretch Goal #2 (Multiclass)**"]},{"cell_type":"code","metadata":{"id":"L-DW6zt1t7EB","executionInfo":{"status":"ok","timestamp":1601131360188,"user_tz":240,"elapsed":936,"user":{"displayName":"Paul Melchiore","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_3OQDAN9XjmBHCwEbbZzQBc37e-CYETEf8Vlldg=s64","userId":"01738411429294267759"}},"outputId":"528ff2dc-74cb-4b6a-d7c4-d949e53ff138","colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from numpy import random\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import StandardScaler"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"zVVfr8xcuIWJ"},"source":["columns = ['sepal length','sepal width','petal length', 'petal width','Classification']\n","dsiris = pd.read_csv('iris.data', sep=',', delimiter=None, header='infer', names=columns)\n","\n","x = dsiris.iloc[:, :-1].values\n","Y = dsiris.iloc[:, -1].values\n","\n","\n","xtrain, xtest, ytrain, ytest = train_test_split(x, Y, test_size = 0.2, random_state = 1)\n","\n","sc = StandardScaler()\n","xtrain = sc.fit_transform(xtrain)\n","xtest = sc.transform(xtest)\n","\n","xtrain = np.array(xtrain)\n","ones = np.ones((1,len(xtrain)))\n","xtrain = np.insert(xtrain, 0, ones, axis=1)\n","\n","xtest = np.array(xtest)\n","ones = np.ones((1,len(xtest)))\n","xtest = np.insert(xtest, 0, ones, axis=1)\n","\n","#making weight vector\n","weights = [[0.1, 0.1,0.1],[0.1, 0.2,0.3],\n","           [0.1, 0.2,0.3],[0.1, 0.2,0.3],\n","           [0.1, 0.2,0.3]]\n","weights = np.array(weights)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MwokfpfRth1w"},"source":["**Above, the code is split and an extra column is added to the feature matricies to account for the bias. The weight matrix is made with dimensions equal to the number of features by number of classes.**"]},{"cell_type":"code","metadata":{"id":"GjqkLrKw1NhE","executionInfo":{"status":"ok","timestamp":1601131360190,"user_tz":240,"elapsed":893,"user":{"displayName":"Paul Melchiore","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_3OQDAN9XjmBHCwEbbZzQBc37e-CYETEf8Vlldg=s64","userId":"01738411429294267759"}},"outputId":"31175cce-cc0c-48e5-d71d-2ca685cb39eb","colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["columns = ['Bias', 'SL','SW','PL','PW']\n","xtrain = pd.DataFrame(xtrain, columns= columns)\n","xtrain.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Bias</th>\n","      <th>SL</th>\n","      <th>SW</th>\n","      <th>PL</th>\n","      <th>PW</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.0</td>\n","      <td>0.315537</td>\n","      <td>-0.036122</td>\n","      <td>0.447486</td>\n","      <td>0.234531</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.0</td>\n","      <td>2.244933</td>\n","      <td>-0.036122</td>\n","      <td>1.298040</td>\n","      <td>1.396429</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.0</td>\n","      <td>-0.287400</td>\n","      <td>-1.240184</td>\n","      <td>0.050561</td>\n","      <td>-0.152768</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.0</td>\n","      <td>0.677298</td>\n","      <td>-0.517747</td>\n","      <td>1.014522</td>\n","      <td>1.138229</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.0</td>\n","      <td>-0.046225</td>\n","      <td>-0.517747</td>\n","      <td>0.731004</td>\n","      <td>1.525529</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Bias        SL        SW        PL        PW\n","0   1.0  0.315537 -0.036122  0.447486  0.234531\n","1   1.0  2.244933 -0.036122  1.298040  1.396429\n","2   1.0 -0.287400 -1.240184  0.050561 -0.152768\n","3   1.0  0.677298 -0.517747  1.014522  1.138229\n","4   1.0 -0.046225 -0.517747  0.731004  1.525529"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"anQdDRHpuK7k"},"source":["setosa = np.array('Iris-setosa')\n","versicolor = np.array('Iris-versicolor')\n","\n","iris = []\n","for i in range(0, len(ytrain)):\n","  if ytrain[i] == 'Iris-setosa':\n","    h = [1,0,0]\n","  elif ytrain[i] == 'Iris-versicolor':\n","    h = [0,1,0]\n","  else:\n","    h = [0,0,1]\n","  iris.append(h)\n","ytrain = iris\n","ytrain = np.array(ytrain)\n","\n","\n","iris1 = []\n","for i in range(0, len(ytest)):\n","  if ytest[i] == 'Iris-setosa':\n","    h = [1,0,0]\n","  elif ytest[i] == 'Iris-versicolor':\n","    h = [0,1,0]\n","  else:\n","    h = [0,0,1]\n","  iris1.append(h)\n","ytest = iris1\n","ytest = np.array(ytest)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_StJvz2Cuqsj"},"source":["**Above, the class we are trying to predict is one hot encoded with the first number for 'Iris-setosa', the second number for 'Iris-versicolor', and the third for 'Iris-virginica'**"]},{"cell_type":"code","metadata":{"id":"tYLHiQXkuOSD"},"source":["a = 0.1\n","#Finding Softmax\n","def softmax(x,w,j):\n","  sum = 0\n","  for i in range(0,3):\n","    sum = sum + np.exp(np.dot(np.transpose(w[:,i]),x))\n","  stuff0 = np.exp(np.dot(np.transpose(w[:,0]),x))/sum\n","  stuff1 = np.exp(np.dot(np.transpose(w[:,1]),x))/sum\n","  stuff2 = np.exp(np.dot(np.transpose(w[:,2]),x))/sum\n","  stuff = np.array([stuff0,stuff1,stuff2])\n","  return stuff[j]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_5QWzEG6vUvD"},"source":["**The softmax function above takes in the current matricies for the weights and a sample from the train matrix. It outputs the probability that the sample is one of the classes with j representing the class.**"]},{"cell_type":"markdown","metadata":{"id":"du58nxgL34j6"},"source":["**The multinomial classification model is computed below. The model is made using each of the samples in the train data 1000 times. Once the model is made, it is used to predict the classes of the xtest data and compare them to the actual classes.**"]},{"cell_type":"code","metadata":{"id":"Yh5vIKxzi-lg","executionInfo":{"status":"ok","timestamp":1601131375416,"user_tz":240,"elapsed":16069,"user":{"displayName":"Paul Melchiore","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_3OQDAN9XjmBHCwEbbZzQBc37e-CYETEf8Vlldg=s64","userId":"01738411429294267759"}},"outputId":"902ef9fc-a459-4eef-b02f-f6c8552a955b","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["a = 0.1\n","g = np.zeros(5)\n","prob = []\n","bestval = 0;\n","num = 0\n","xtrain = np.array(xtrain)\n","\n","#--------------Training Model using train data-------------#\n","for k in range(0, 1000):\n","  for i in range (0,len(xtrain)):\n","    for j in range(0,3):\n","      h = softmax(xtrain[i],weights,j)\n","      prob.append(h)\n","    for j in range(0,3):\n","      t = ytrain[i][j]\n","      o = prob[j]\n","      g = g + (-1/(len(xtrain)) * xtrain[i] *(t-o))\n","      weights[:,j] = weights[:,j] - (a * g) \n","    prob = []\n","#print(weights)\n","\n","yguess = []\n","predguess = []\n","probT = []\n","\n","#------Determining effectiveness of model on test data-----#\n","\n","for i in range (0,len(xtest)):\n","  for j in range(0,3):\n","    check = softmax(xtest[i],weights,j)\n","    probT.append(check)\n","\n","  predguess.append(np.argmax(probT))\n","  yguess.append(np.argmax(ytest[i]))\n"," \n","  probT = []\n","score = accuracy_score(yguess, predguess)\n","yguess = np.array(yguess)\n","predguess = np.array(predguess)\n","#print(np.concatenate((predguess.reshape(len(predguess),1), yguess.reshape(len(yguess),1)),1))\n","print(\"Accuracy Score: \", score)\n","\n","baseline = np.sum([0 == np.argmax(ytest[i]) for i in range(0, len(ytest))])\n","ascore_baseline = baseline/len(ytest)\n","print(\"Accuracy Score of baseline: \", ascore_baseline)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy Score:  0.9666666666666667\n","Accuracy Score of baseline:  0.36666666666666664\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NSYCxC4cxkrj"},"source":["**With this high of an accuracy, it is clear that the model is really good at approximating the outcome of the iris dataset. It only messed up once. This is much more effective than using the baseline model.**"]}]}